#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
create_RAG.py - TSCG RAG System avec syntaxe simple
Usage:
  python create_RAG.py api          # Utilise Google Gemini API
  python create_RAG.py local        # Utilise SentenceTransformers local
  python create_RAG.py --help       # Affiche l'aide dÃ©taillÃ©e
"""

import os
import sys
import time
import hashlib
import glob
import argparse
from typing import List
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain_core.documents import Document

# ---------------------------------------------------------------------------
# 0. Parser d'arguments avec syntaxe api/local
# ---------------------------------------------------------------------------
def parse_arguments():
    """Parse les arguments de ligne de commande avec syntaxe api/local"""
    parser = argparse.ArgumentParser(
        description="TSCG RAG System - CrÃ©e une base de connaissances vectorielle",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
EXEMPLES:
  python create_RAG.py api           # Utilise Google Gemini API (besoin de clÃ©)
  python create_RAG.py local         # Utilise SentenceTransformers local (gratuit)
  
  python create_RAG.py api --update  # Mise Ã  jour avec API Google
  python create_RAG.py local --test  # Teste avec embeddings locaux
  
  python create_RAG.py api --extensions md txt --chunk-size 2000
  python create_RAG.py local --no-llm  # Uniquement embeddings, pas de LLM
  
  python create_RAG.py --version     # Affiche la version
  python create_RAG.py --help        # Affiche cette aide

CONSEILS:
  â€¢ Utilisez 'local' si vous dÃ©passez les quotas Google
  â€¢ Utilisez 'api' pour de meilleurs embeddings (quota limitÃ©)
  â€¢ Placez votre clÃ© API dans ../.api_key ou ./.api_key
        """
    )
    
    # Argument principal (api ou local)
    parser.add_argument(
        "mode",
        choices=["api", "local"],
        help="Mode d'exÃ©cution: 'api' pour Google Gemini, 'local' pour SentenceTransformers"
    )
    
    # Options gÃ©nÃ©rales
    parser.add_argument(
        "--update", 
        action="store_true",
        help="Mode mise Ã  jour incrÃ©mentielle (ajoute seulement les nouveaux fichiers)"
    )
    
    parser.add_argument(
        "--test", 
        action="store_true",
        help="Mode test (vÃ©rifie le systÃ¨me sans ajouter de documents)"
    )
    
    parser.add_argument(
        "--extensions", 
        nargs="+",
        default=["md", "jsonld", "txt", "cs", "fs"],
        help="Extensions de fichiers Ã  indexer (dÃ©faut: md jsonld txt cs fs)"
    )
    
    parser.add_argument(
        "--chunk-size", 
        type=int,
        default=1500,
        help="Taille des chunks en caractÃ¨res (dÃ©faut: 1500)"
    )
    
    parser.add_argument(
        "--chunk-overlap", 
        type=int,
        default=200,
        help="Chevauchement entre chunks (dÃ©faut: 200)"
    )
    
    parser.add_argument(
        "--no-llm", 
        action="store_true",
        help="DÃ©sactive le LLM (utilise uniquement les embeddings)"
    )
    
    parser.add_argument(
        "--api-key", 
        type=str,
        metavar="CHEMIN",
        help="Chemin vers le fichier .api_key (dÃ©faut: cherche dans ../.api_key ou ./.api_key)"
    )
    
    parser.add_argument(
        "--interactive",
        action="store_true",
        help="Lance directement le mode interactif aprÃ¨s la crÃ©ation"
    )
    
    parser.add_argument(
        "--version",
        action="version",
        version="TSCG RAG System v2.0 (FÃ©vrier 2026)"
    )
    
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Affiche plus d'informations de debug"
    )
    
    return parser.parse_args()

# ---------------------------------------------------------------------------
# 1. Chargement de la clÃ© API
# ---------------------------------------------------------------------------
def load_api_key(api_key_path=None, verbose=False):
    """Charge la clÃ© API Gemini depuis le chemin spÃ©cifiÃ© ou par dÃ©faut"""
    possible_paths = []
    
    # Chemin spÃ©cifiÃ© par l'utilisateur
    if api_key_path:
        possible_paths.append(api_key_path)
    
    # Chemins par dÃ©faut
    possible_paths.extend(["../.api_key", "./.api_key"])
    
    for path in possible_paths:
        if os.path.exists(path):
            try:
                with open(path, "r", encoding="utf-8") as f:
                    key = f.read().strip()
                    os.environ["GOOGLE_API_KEY"] = key
                    if verbose:
                        print(f"âœ“ ClÃ© API chargÃ©e depuis : {path}")
                    else:
                        print(f"âœ“ ClÃ© API chargÃ©e")
                    return key
            except Exception as e:
                if verbose:
                    print(f"âš ï¸  Erreur lecture {path}: {e}")
                continue
    
    print("âš ï¸  Fichier .api_key non trouvÃ©. Le LLM ne sera pas disponible.")
    print("   Vous pouvez quand mÃªme utiliser les embeddings locaux.")
    return None

# ---------------------------------------------------------------------------
# 2. Gestion des embeddings selon le mode
# ---------------------------------------------------------------------------
def get_embeddings(mode, verbose=False):
    """Retourne le modÃ¨le d'embeddings selon le mode choisi"""
    if mode == "local":
        print("ğŸ”§ Configuration des embeddings LOCAUX (SentenceTransformers)...")
        try:
            # Essayer d'importer
            from langchain_huggingface import HuggingFaceEmbeddings
            if verbose:
                print("âœ“ SentenceTransformers trouvÃ©")
        except ImportError:
            print("ğŸ“¦ Installation de sentence-transformers...")
            import subprocess
            try:
                subprocess.check_call([sys.executable, "-m", "pip", "install", "sentence-transformers"])
                from langchain_huggingface import HuggingFaceEmbeddings
                print("âœ“ SentenceTransformers installÃ©")
            except Exception as e:
                print(f"âŒ Ã‰chec installation: {e}")
                print("   ExÃ©cutez: pip install sentence-transformers")
                return None
        
        return HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
    
    else:  # mode == "api"
        print("ğŸ”§ Configuration des embeddings GOOGLE API (Gemini)...")
        try:
            from langchain_google_genai import GoogleGenerativeAIEmbeddings
            return GoogleGenerativeAIEmbeddings(model="models/gemini-embedding-001")
        except Exception as e:
            print(f"âŒ Erreur chargement Google embeddings: {e}")
            print("   Utilisez 'python create_RAG.py local' pour les embeddings locaux")
            return None

# ---------------------------------------------------------------------------
# 3. Custom document loader
# ---------------------------------------------------------------------------
class TSCGDocumentLoader:
    def __init__(self, file_path: str):
        self.file_path = file_path
    
    def load(self) -> List[Document]:
        try:
            file_path = os.path.normpath(self.file_path)
            
            encodings = ['utf-8', 'latin-1', 'cp1252']
            
            for encoding in encodings:
                try:
                    with open(file_path, 'r', encoding=encoding, errors='replace') as f:
                        content = f.read()
                    
                    if content and len(content.strip()) > 0:
                        metadata = {
                            "source": file_path,
                            "filename": os.path.basename(file_path),
                            "size": len(content),
                            "encoding": encoding
                        }
                        return [Document(page_content=content, metadata=metadata)]
                except UnicodeDecodeError:
                    continue
            
            # Fallback binaire
            with open(file_path, 'rb') as f:
                binary_content = f.read()
                try:
                    content = binary_content.decode('utf-8', errors='replace')
                except:
                    content = binary_content.decode('latin-1', errors='replace')
                
                metadata = {
                    "source": file_path,
                    "filename": os.path.basename(file_path),
                    "size": len(content),
                    "encoding": "fallback"
                }
                return [Document(page_content=content, metadata=metadata)]
                
        except Exception as e:
            print(f"  âš ï¸  Impossible de charger {self.file_path}: {type(e).__name__}")
            return []

# ---------------------------------------------------------------------------
# 4. Safe directory loader
# ---------------------------------------------------------------------------
def load_documents_from_directory(
    root_dir: str = "..",
    extensions: List[str] = None,
    ignored_patterns: List[str] = None,
    verbose: bool = False
) -> List[Document]:
    
    if extensions is None:
        extensions = ["*.md", "*.jsonld", "*.txt"]
    
    if ignored_patterns is None:
        ignored_patterns = ["obj\\", "bin\\", "_archives\\", ".git\\", ".api_key", "RAG\\"]
    
    all_docs = []
    ignored_patterns = [p.replace('\\', os.path.sep) for p in ignored_patterns]
    
    for ext in extensions:
        pattern = os.path.join(root_dir, "**", ext)
        files = glob.glob(pattern, recursive=True)
        
        if verbose:
            print(f"  Recherche {ext}... {len(files)} fichiers trouvÃ©s")
        
        for file_path in files:
            # Ignorer les chemins spÃ©cifiÃ©s
            skip = False
            for pattern in ignored_patterns:
                if pattern in file_path:
                    skip = True
                    break
            
            if skip:
                if verbose:
                    print(f"  IgnorÃ©: {file_path}")
                continue
            
            try:
                loader = TSCGDocumentLoader(file_path)
                docs = loader.load()
                if docs:
                    all_docs.extend(docs)
                    if verbose:
                        print(f"  âœ“ ChargÃ©: {os.path.basename(file_path)}")
            except Exception as e:
                if verbose:
                    print(f"  âš ï¸  Erreur chargement {file_path}: {type(e).__name__}")
                continue
    
    return all_docs

# ---------------------------------------------------------------------------
# 5. Utilitaires
# ---------------------------------------------------------------------------
def _chunk_hash(doc: Document) -> str:
    raw = f"{doc.metadata.get('source', '')}::{doc.page_content}"
    return hashlib.sha256(raw.encode("utf-8")).hexdigest()

def format_documents(docs: List[Document]) -> str:
    return "\n\n".join([doc.page_content for doc in docs])

# ---------------------------------------------------------------------------
# 6. Fonction principale
# ---------------------------------------------------------------------------
def build_tscg_rag(args):
    """Fonction principale avec paramÃ¨tres"""
    
    print("\n" + "="*60)
    print("ğŸš€ TSCG RAG System")
    print("="*60)
    print(f"Mode: {'GOOGLE API' if args.mode == 'api' else 'LOCAL (SentenceTransformers)'}")
    if args.update:
        print("ğŸ”„ Mode: MISE Ã€ JOUR (incrÃ©mentiel)")
    if args.test:
        print("ğŸ§ª Mode: TEST (lecture seule)")
    if args.no_llm:
        print("ğŸ¤– LLM: DÃ‰SACTIVÃ‰")
    print("="*60)
    
    # Charger la clÃ© API (nÃ©cessaire pour le LLM, mÃªme en mode local)
    api_key = load_api_key(args.api_key, args.verbose)
    
    # Initialiser les embeddings selon le mode
    embeddings = get_embeddings(args.mode, args.verbose)
    if embeddings is None:
        print("âŒ Ã‰chec initialisation des embeddings.")
        return None
    
    # --- Chargement des documents ------------------------------------------
    if not args.test:
        print("\nğŸ“‚ Scan des documents...")
        
        # Convertir les extensions en pattern glob
        extensions = [f"*.{ext}" if not ext.startswith("*") else ext for ext in args.extensions]
        
        all_docs = load_documents_from_directory(
            root_dir="..",
            extensions=extensions,
            ignored_patterns=["obj\\", "bin\\", "_archives\\", ".git\\", ".api_key", "RAG\\"],
            verbose=args.verbose
        )
        
        if not all_docs:
            print("  âŒ Aucun document trouvÃ© avec les extensions spÃ©cifiÃ©es.")
            return None
        
        print(f"  âœ“ {len(all_docs)} fichier(s) source chargÃ©(s).")
        
        # Afficher un Ã©chantillon
        if len(all_docs) > 0 and args.verbose:
            sample_files = list(set([doc.metadata.get("source", "inconnu") for doc in all_docs[:5]]))
            print(f"  Ã‰chantillon: {', '.join([os.path.basename(f) for f in sample_files])}")
        
        # --- DÃ©coupage en chunks -------------------------------------------
        print("\nğŸ”ª DÃ©coupage des documents...")
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=args.chunk_size, 
            chunk_overlap=args.chunk_overlap,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
        chunks = splitter.split_documents(all_docs)
        print(f"  âœ“ {len(chunks)} chunk(s) crÃ©Ã©(s).")
        
        if args.verbose and chunks:
            avg_size = sum(len(c.page_content) for c in chunks) / len(chunks)
            print(f"  Taille moyenne: {avg_size:.0f} caractÃ¨res")
    else:
        print("\nâ¸ï¸  MODE TEST - Pas de chargement/dÃ©coupage")
        chunks = []
    
    # --- Base de donnÃ©es vectorielle ---------------------------------------
    print("\nğŸ’¾ Configuration base de donnÃ©es vectorielle...")
    db_dir = "./db_vector"
    os.makedirs(db_dir, exist_ok=True)
    
    try:
        vectorstore = Chroma(
            embedding_function=embeddings,
            persist_directory=db_dir,
            collection_name="tscg"
        )
        print(f"  âœ“ Base de donnÃ©es prÃªte: {db_dir}")
    except Exception as e:
        print(f"âŒ Erreur base de donnÃ©es: {e}")
        return None
    
    # --- VÃ©rification des doublons -----------------------------------------
    if not args.test:
        print("\nğŸ” VÃ©rification doublons...")
        try:
            existing_data = vectorstore.get(include=["metadatas", "documents"])
            
            indexed_hashes: set[str] = set()
            if existing_data["documents"]:
                for doc_text, meta in zip(existing_data["documents"],
                                          existing_data["metadatas"]):
                    dummy = Document(
                        page_content=doc_text,
                        metadata=meta if meta else {}
                    )
                    indexed_hashes.add(_chunk_hash(dummy))
            print(f"  âœ“ Base contient {len(indexed_hashes)} chunk(s) existant(s).")
        except Exception as e:
            if args.verbose:
                print(f"  â„¹ï¸  Pas d'index existant: {type(e).__name__}")
            indexed_hashes = set()
        
        # --- Ajout des nouveaux chunks -------------------------------------
        new_chunks = [c for c in chunks if _chunk_hash(c) not in indexed_hashes]
        skipped = len(chunks) - len(new_chunks)
        
        if skipped:
            print(f"  â­ï¸  {skipped} chunk(s) dÃ©jÃ  indexÃ©(s) ignorÃ©(s).")
        
        if not new_chunks:
            print("\nâœ… Base Ã  jour - aucun ajout nÃ©cessaire.")
        else:
            print(f"\nğŸ”„ Ajout de {len(new_chunks)} nouveau(x) chunk(s)...")
            
            # Taille des lots selon le mode
            batch_size = 50 if args.mode == "local" else 5  # Plus grand si local
            total_added = 0
            
            for i in range(0, len(new_chunks), batch_size):
                batch = new_chunks[i : i + batch_size]
                batch_num = i // batch_size + 1
                total_batches = (len(new_chunks) + batch_size - 1) // batch_size
                
                if args.mode == "api":  # Rate limiting pour Google
                    print(f"    Lot {batch_num}/{total_batches} - Pause 15s pour quota API...")
                    time.sleep(15)
                
                try:
                    vectorstore.add_documents(batch)
                    total_added += len(batch)
                    progress = min(i + batch_size, len(new_chunks))
                    print(f"    [{progress}/{len(new_chunks)}] âœ“ Lot {batch_num} ajoutÃ© ({len(batch)} chunks)")
                    
                except Exception as e:
                    error_str = str(e)
                    if "429" in error_str or "rate limit" in error_str.lower():
                        print(f"\nâš ï¸  Limite de quota atteinte. {total_added} chunk(s) ajoutÃ©(s).")
                        print("   Relancez plus tard pour continuer.")
                        break
                    elif "quota" in error_str.lower():
                        print(f"\nâš ï¸  Quota API Ã©puisÃ©. {total_added} chunk(s) ajoutÃ©(s).")
                        print("   Attendez 24h ou utilisez 'python create_RAG.py local'")
                        break
                    else:
                        print(f"âŒ Erreur lot {batch_num}: {type(e).__name__}")
                        if args.verbose:
                            print(f"   DÃ©tails: {e}")
                        break
            
            print(f"\nâœ… {total_added} nouveau(x) chunk(s) ajoutÃ©(s).")
    
    # --- Configuration du LLM (optionnel) ----------------------------------
    rag_chain = None
    if not args.no_llm and api_key:
        print("\nğŸ¤– Configuration LLM (Gemini)...")
        try:
            llm = ChatGoogleGenerativeAI(
                model="gemini-2.5-flash",
                temperature=0,
                max_retries=2
            )
            
            template = """Vous Ãªtes l'assistant TSCG. RÃ©pondez UNIQUEMENT basÃ© sur le contexte.

Contexte:
{context}

Question: {question}

RÃ©pondez dans la mÃªme langue que la question. Si l'information n'est pas dans le contexte, dites-le."""

            prompt = ChatPromptTemplate.from_template(template)

            rag_chain = (
                {
                    "context": vectorstore.as_retriever(search_kwargs={"k": 8}) | RunnableLambda(format_documents),
                    "question": RunnablePassthrough()
                }
                | prompt
                | llm
                | StrOutputParser()
            )
            
            print("  âœ“ ChaÃ®ne RAG prÃªte")
            
            # Test simple
            if not args.test:
                print("  ğŸ§ª Test de la chaÃ®ne RAG...")
                try:
                    test_query = "Qu'est-ce que TSCG?"
                    test_result = rag_chain.invoke(test_query)
                    if test_result and len(test_result.strip()) > 10:
                        print(f"  âœ… Test rÃ©ussi")
                        if args.verbose:
                            print(f"  Extrait: {test_result[:150]}...")
                    else:
                        print(f"  âš ï¸  RÃ©ponse minimale du test")
                except Exception as e:
                    print(f"  âš ï¸  Test LLM Ã©chouÃ©: {type(e).__name__}")
                    if args.verbose:
                        print(f"     DÃ©tails: {e}")
        
        except Exception as e:
            print(f"âŒ Configuration LLM Ã©chouÃ©e: {type(e).__name__}")
            if args.verbose:
                print(f"   DÃ©tails: {e}")
            print("   Utilisation des embeddings seulement.")
    
    return rag_chain, vectorstore, args

# ---------------------------------------------------------------------------
# 7. Mode interactif
# ---------------------------------------------------------------------------
def interactive_mode(rag_chain, vectorstore, args):
    """Mode interactif pour poser des questions"""
    if rag_chain is None:
        print("\nâš ï¸  LLM non disponible. Mode interactif impossible.")
        print("   VÃ©rifiez votre clÃ© API ou utilisez sans --no-llm.")
        return
    
    print("\n" + "="*60)
    print("ğŸ§  TSCG RAG - Mode Interactif")
    print("="*60)
    print("Posez des questions sur TSCG en franÃ§ais ou anglais")
    print("Commandes: 'exit'=quitter, 'stats'=statistiques, 'sources'=fichiers")
    print("="*60)
    
    while True:
        try:
            query = input("\nâ“ Question: ").strip()
            
            if not query:
                continue
                
            if query.lower() in ['exit', 'quit', 'q']:
                print("\nğŸ‘‹ Au revoir !")
                break
            
            if query.lower() == 'stats':
                try:
                    data = vectorstore.get()
                    count = len(data['documents']) if data['documents'] else 0
                    print(f"\nğŸ“Š Statistiques base:")
                    print(f"  Chunks totaux: {count}")
                    print(f"  Mode embeddings: {'GOOGLE API' if args.mode == 'api' else 'LOCAL'}")
                    print(f"  LLM disponible: OUI")
                    
                    if data["metadatas"]:
                        import os
                        from collections import Counter
                        file_types = Counter()
                        for meta in data["metadatas"]:
                            if meta and "source" in meta:
                                ext = os.path.splitext(meta["source"])[1].lower()
                                file_types[ext] += 1
                        
                        print(f"\n  Types de fichiers:")
                        for ext, cnt in file_types.most_common(5):
                            print(f"    {ext}: {cnt} chunks")
                            
                except Exception as e:
                    print(f"âš ï¸  Impossible statistiques: {e}")
                continue
            
            if query.lower() == 'sources':
                try:
                    data = vectorstore.get()
                    sources = set()
                    for meta in data["metadatas"]:
                        if meta and "source" in meta:
                            sources.add(os.path.basename(meta["source"]))
                    
                    print(f"\nğŸ“š Fichiers sources ({len(sources)}):")
                    for i, source in enumerate(sorted(list(sources))[:15], 1):
                        print(f"  {i:2}. {source}")
                    if len(sources) > 15:
                        print(f"  ... et {len(sources) - 15} autres")
                except Exception as e:
                    print(f"âš ï¸  Impossible sources: {e}")
                continue
            
            print("ğŸ” Recherche...")
            try:
                response = rag_chain.invoke(query)
                print(f"\nğŸ’¡ RÃ©ponse:\n{'-'*40}")
                print(response)
                print('-'*40)
            except Exception as e:
                print(f"\nâŒ Erreur: {type(e).__name__}")
                if "429" in str(e) or "quota" in str(e).lower():
                    print("   Quota API Ã©puisÃ©. Essayez plus tard.")
                elif "api key" in str(e).lower():
                    print("   ProblÃ¨me clÃ© API. VÃ©rifiez votre fichier .api_key")
            
        except KeyboardInterrupt:
            print("\n\nâš ï¸  Interrompu. Tapez 'exit' pour quitter.")
        except Exception as e:
            print(f"\nâŒ Erreur entrÃ©e: {e}")

# ---------------------------------------------------------------------------
# 8. Point d'entrÃ©e principal
# ---------------------------------------------------------------------------
def main():
    """Fonction principale"""
    # Parser les arguments
    args = parse_arguments()
    
    # Construction du systÃ¨me RAG
    result = build_tscg_rag(args)
    
    if result is None:
        print("\nâŒ CrÃ©ation systÃ¨me RAG Ã©chouÃ©e.")
        sys.exit(1)
    
    rag_chain, vectorstore, args = result
    
    # RÃ©sumÃ© final
    print("\n" + "="*60)
    if rag_chain:
        print("âœ… SystÃ¨me TSCG RAG PrÃªt ! (RAG Complet)")
    else:
        print("âœ… Base embeddings TSCG PrÃªte ! (LLM non disponible)")
    print("="*60)
    
    # Statistiques finales
    try:
        data = vectorstore.get()
        total_chunks = len(data["documents"]) if data["documents"] else 0
        
        print(f"\nğŸ“Š Statistiques finales:")
        print(f"  Chunks totaux: {total_chunks}")
        print(f"  Mode: {'GOOGLE API' if args.mode == 'api' else 'LOCAL (SentenceTransformers)'}")
        print(f"  LLM: {'DISPONIBLE' if rag_chain else 'NON DISPONIBLE'}")
        print(f"  Base de donnÃ©es: ./db_vector/")
        
        if total_chunks > 0 and args.verbose:
            total_chars = sum(len(doc) for doc in data["documents"])
            avg_size = total_chars / total_chunks
            print(f"  Taille moyenne chunk: {avg_size:.0f} caractÃ¨res")
            print(f"  CaractÃ¨res totaux: {total_chars:,}")
    
    except Exception as e:
        print(f"âš ï¸  Impossible statistiques finales: {e}")
    
    # Mode interactif automatique ou sur demande
    if rag_chain and (args.interactive or not args.test):
        if args.interactive:
            interactive_mode(rag_chain, vectorstore, args)
        else:
            choice = input("\nğŸ® Mode interactif ? (o/n): ").strip().lower()
            if choice in ['o', 'oui', 'y', 'yes']:
                interactive_mode(rag_chain, vectorstore, args)
    
    print("\n" + "="*60)
    print("ğŸ‘‹ TerminÃ© !")
    print("="*60)

# ---------------------------------------------------------------------------
# 9. Gestion des erreurs
# ---------------------------------------------------------------------------
if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ Interrompu par l'utilisateur. Au revoir !")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ Erreur inattendue: {type(e).__name__}: {e}")
        if "--verbose" in sys.argv:
            import traceback
            traceback.print_exc()
        sys.exit(1)